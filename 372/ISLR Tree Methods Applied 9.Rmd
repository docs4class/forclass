---
title: "ISLR Tree Methods Applied #9"
author: "Tobin Turner"
date: "2/24/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#a

library(ISLR)
attach(OJ)
set.seed(1013)

train = sample(dim(OJ)[1], 800)
OJ.train = OJ[train, ]
OJ.test = OJ[-train, ]

#b

library(tree)
oj.tree = tree(Purchase ~ ., data = OJ.train)
summary(oj.tree)

# The tree only uses two variables: LoyalCH
# and PriceDiff. It has 7 terminal nodes. Training error rate (misclassification error) for the tree is 0.155

#c

oj.tree

# Let's pick terminal node labeled “10)”. The splitting variable at this node is PriceDiff
# . The splitting value of this node is 0.05. There are 79 points in the subtree below this node. The deviance for all points contained in region below this node is 80. A * in the line denotes that this is in fact a terminal node. The prediction at this node is Sales = MM. About 19% points in this node have CH as value of Sales. Remaining 81% points have MM as value of Sales

# d

plot(oj.tree)
text(oj.tree, pretty = 0)

# LoyalCH
# is the most important variable of the tree, in fact top 3 nodes contain LoyalCH. If LoyalCH<0.27, the tree predicts MM. If LoyalCH>0.76, the tree predicts CH. For intermediate values of LoyalCH, the decision also depends on the value of PriceDiff
 
# e

oj.pred = predict(oj.tree, OJ.test, type = "class")
table(OJ.test$Purchase, oj.pred)

# f

cv.oj = cv.tree(oj.tree, FUN = prune.tree)

# g

plot(cv.oj$size, cv.oj$dev, type = "b", xlab = "Tree Size", ylab = "Deviance")

# h

# Size of 6 gives lowest cross-validation error.
# i

oj.pruned = prune.tree(oj.tree, best = 6)

# j

summary(oj.pruned)

# Misclassification error of pruned tree is exactly same as that of original tree — 0.155

#  k

pred.unpruned = predict(oj.tree, OJ.test, type = "class")
misclass.unpruned = sum(OJ.test$Purchase != pred.unpruned)
misclass.unpruned/length(pred.unpruned)

## [1] 0.1889

pred.pruned = predict(oj.pruned, OJ.test, type = "class")
misclass.pruned = sum(OJ.test$Purchase != pred.pruned)
misclass.pruned/length(pred.pruned)

## [1] 0.1889

# Pruned and unpruned trees have same test error rate of 0.189

```

